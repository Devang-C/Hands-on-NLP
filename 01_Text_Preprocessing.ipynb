{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing in NLP\n",
    "\n",
    "Text preprocessing is a crucial step while building NLP solutions and applications. Once we acquire some data for our project, the first thing we need to perform is to do text preprocessing to make sure that our data is suitable for input to our machine learning / deep learning model. It makes sure that the data is consistent, does not contain unecessary things and data is as per the project requirements\n",
    "\n",
    "## Basic Steps in Text Preprocessing\n",
    "\n",
    "1. Lowercasing\n",
    "2. Remove HTML Tags\n",
    "3. Remove URLs\n",
    "4. Remove Punctuations\n",
    "5. Chat word treatment\n",
    "6. Spelling Correction\n",
    "7. Removing stop words\n",
    "8. Handling emojis\n",
    "9. Tokenization\n",
    "10. Stemming\n",
    "11. Lammatization\n",
    "\n",
    "It is not necessary to apply all the above steps to our dataset. We need to apply common sense and make decisions on our own to apply proper text preprocessing as per the project requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('Datasets/IMDB Dataset.csv')\n",
    "\n",
    "display(data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. It just never gets old, despite my having seen it some 15 or more times in the last 25 years. Paul Lukas\\' performance brings tears to my eyes, and Bette Davis, in one of her very few truly sympathetic roles, is a delight. The kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. And the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they\\'d all be \"up\" for this movie.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To lowercase a text in python we simply use str.lower\n",
    "\n",
    "# to lowercase the entire review column in our dataset we will do the following\n",
    "data['review'] = data['review'].str.lower()\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove HTML Tags\n",
    "HTML tags generally come when we scrape data from any website using an web scraping tools such as beautifulSoap or something else. These tags aren't required for training a machine learning or deep learning model. Hence we will remove the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs\n",
    "\n",
    "URLs can be present in the text if we have some data from say facebook or twitter or instagram. These urls are again not very useful for our model and in fact can cause confusion for our model. Hence we will remove the urls from our text using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For notebook click  to search check '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'\n",
    "\n",
    "remove_urls(text4)\n",
    "# our dataset does not contain any urls as such, hence we'll not apply this to our data. But if we wish to, we can simpy use .apply to apply our remove_urls function to our reviews column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation # this gives all the punctuation marks in the english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~' # did not take full stop(.) because it may be necessary. we can change this string as per our requirement\n",
    "# this below code is our own logic developed from scratch. hence, it isn't optimized and hence would take a long time to execute on large datasets.\n",
    "def remove_punctuation(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_punctuation_optimized(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically theres a family where a little boy j...  negative\n",
       "4  petter matteis love in the time of money is a ...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(remove_punctuation_optimized)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat word treatment\n",
    "\n",
    "In many chat applications, we use chat words such as fyi(for your information), imho(in my honest opinion) etc etc. It becomes difficult for our model to understand these words and hence, we need to treat these words in a manner that our model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we found a repo on github that contains a dictionary of the words and their full forms. We will use this \n",
    "slang_dict = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': 'For What It\\'s Worth',\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you (also a chat program)',\n",
    "    'ILU': 'I Love You',\n",
    "    'IMHO': 'In My Honest/Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My A.. Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'PITA': 'Pain In The A..',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The F...',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait...',\n",
    "    '7K': 'Sick:-D Laughter',\n",
    "    'TFW': 'That feeling when',\n",
    "    'MFW': 'My face when',\n",
    "    'MRW': 'My reaction when',\n",
    "    'IFYP': 'I feel your pain',\n",
    "    'LOL': 'Laughing out loud',\n",
    "    'TNTL': 'Trying not to laugh',\n",
    "    'JK': 'Just kidding',\n",
    "    'IDC': 'I donâ€™t care',\n",
    "    'ILY': 'I love you',\n",
    "    'IMU': 'I miss you',\n",
    "    'ADIH': 'Another day in hell',\n",
    "    'IDC': 'I donâ€™t care',\n",
    "    'ZZZ': 'Sleeping, bored, tired',\n",
    "    'WYWH': 'Wish you were here',\n",
    "    'TIME': 'Tears in my eyes',\n",
    "    'BAE': 'Before anyone else',\n",
    "    'FIMH': 'Forever in my heart',\n",
    "    'BSAAW': 'Big smile and a wink',\n",
    "    'BWL': 'Bursting with laughter',\n",
    "    'LMAO': 'Laughing my a** off',\n",
    "    'BFF': 'Best friends forever',\n",
    "    'CSL': 'Canâ€™t stop laughing'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in slang_dict:\n",
    "            new_text.append(slang_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Your Information i already knew'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example use case\n",
    "text_slang = \"FYI i already knew\"\n",
    "chat_conversion(text_slang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text preprocesing is a crucial step in natural language processing'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = 'texxt preprocesing is a curciaal stap in naturla languag procesing'\n",
    "\n",
    "text_blob = TextBlob(incorrect_text)\n",
    "\n",
    "text_blob.correct().string\n",
    "\n",
    "# there are many other ways that we can correct spelling in python. for exapmle we can use a library named pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words\n",
    "\n",
    "These are the words that help in sentence formation, but they contribute very little to the actual meaning of the sentence. For example: the, and, for etc. These words can be easily removed using the NLTK library\n",
    "\n",
    "There are some tasks where we keep the stop words, such as Parts of Speech Tagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english') # gives all the stop words present in the english language. we can similarly do for other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This   really great time   field  AI. It  advancing exponentially'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = 'This is a really great time for the field of AI. It is advancing exponentially'\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to apply this on our dataset, we can simply use the apply function \n",
    "# data['review'] = data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Emojis\n",
    "\n",
    "Emojis are commonly used for expressing sentiment and expressions. But the problem with emojis is that our models dont understand the emojis.EnvironmentError\n",
    "We have 2 options for handling emojis. One can remove the emojis altogether, or else we can replace them with their meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are very funny '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"You are very funny ðŸ˜‚ðŸ˜‚ðŸ˜‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are very funny :face_with_tears_of_joy::face_with_tears_of_joy::face_with_tears_of_joy:\n"
     ]
    }
   ],
   "source": [
    "#remove emojis\n",
    "import emoji\n",
    "print(emoji.demojize('You are very funny ðŸ˜‚ðŸ˜‚ðŸ˜‚'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is basically breaking up our text document into smaller parts known as tokens. The smallers parts can be words or sentences as well\n",
    "\n",
    "Why is tokenization important\n",
    "A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method(very basic): Using the split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Mumbai']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1= 'I am going to Mumbai'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to Mumbai',\n",
       " ' I will stay there for 10 days',\n",
       " ' I hope I enjoy the trip']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = 'I am going to Mumbai. I will stay there for 10 days. I hope I enjoy the trip'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Method: Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'mumbai']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to mumbai!'\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text) # this will split the sentences on the basis of . ? ! all of them\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Method: Use NLTK Library\n",
    "\n",
    "This library already has some good sophisticated alogirthms for performing tokenization to handle difficult cases while tokenizing some document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Mumbai', '!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sent1= 'I am going to Mumbai!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5) # we can see in the output it did a good job in tokenizing difficult words such as Ph.D and A.I while \n",
    "# not mistaking it for 2 different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method four: Using Spacy(works better than NLTK most of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection(it means a same word is expressed in different grammatical categories such as tense, case, voice etc. For example walk,walking,walked or do,undo,doable, undoable etc ) in words to their root forms such as mapping group of words to the same stem even if the stem itself is not a valid word in the language\n",
    "\n",
    "Stemming is used alot in **Information Retrival Systems**\n",
    "\n",
    "Stemmer is an algorithm that performs stemming for us. There are multiple stemmers in the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'walk walks walking walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "It's essentially the same as stemming, but as we know, in the process of stemming, we sometimes get stem words that are not actual words in the english language. To solve this, we perform lemmatization, where the output for lemmatization is always a word from the dictionary. Lemmatization is slower. \n",
    "\n",
    "If we don't want to show the output to the user, we will use stemming. But suppose there is some output text that we want to display to the user, then we will have to use lemmatization.\n",
    "\n",
    "In lemmatization, we will search for the word in a dictionary for example wordnet(it is a lexical dictionary) and hence it is slower as it is not algorithm based like stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
